spec <- ugarchspec(mean.model=list(armaOrder=c(0,0),
include.mean=FALSE),
variance.model=list(model='sGARCH'))
mgarch.spec <- dccspec(multispec(replicate(3,spec )) )
#split the sample
Y <- Rt["2005-01-03::2009-12-31"]
fit<-dccfit(mgarch.spec,data=Y)
fit
## plot conditional variances and conditional correlations
library(ggplot2)
library(dplyr)
library(hrbrthemes)
# install.packages("dplyr")
da1 <- rcov(fit)['GSPC','GSPC',]
da1 <- xts(da1,order.by=as.Date(names(da1)))
p1 <- ggplot(da1,aes(x=index(da1),y=coredata(da1)))+geom_line(color="red")+
xlab("Conditional variance for S&P 500")+ylab("")
da2 <- rcov(fit)['SHY','SHY',]
da2 <- xts(da2,order.by=as.Date(names(da2)))
p2 <- ggplot(da2,aes(x=index(da2),y=coredata(da2)))+geom_line(color="blue")+
xlab("Conditional variance for SHY")+ylab("")
da3 <- rcov(fit)['LQD','LQD',]
da3 <- xts(da3,order.by=as.Date(names(da3)))
p3 <- ggplot(da3,aes(x=index(da3),y=coredata(da3)))+geom_line(color="green")+
xlab("Conditional variance for LQD")+ylab("")
library(cowplot)
plot_grid(p1,p2,p3,labels="",nrow=3)
rm(p1,p2,p3) #rimuove le tre componenti p1,p2,p3
da1 <- rcor(fit,type="R")['GSPC','SHY',]
da1 <- xts(da1,order.by=as.Date(names(da1)))
p1 <- ggplot(da1,aes(x=index(da1),y=coredata(da1)))+geom_line(color="red")+
xlab("Conditional correlation for S&P 500 and SHY")+ylab("")
da2 <- rcor(fit,type="R")['GSPC','LQD',]
da2 <- xts(da2,order.by=as.Date(names(da2)))
p2 <- ggplot(da2,aes(x=index(da2),y=coredata(da2)))+geom_line(color="blue")+
xlab("Conditional correlation for S&P 500 and LQD")+ylab("")
da3 <- rcor(fit,type="R")['SHY','LQD',]
da3 <- xts(da3,order.by=as.Date(names(da3)))
p3 <- ggplot(da3,aes(x=index(da3),y=coredata(da3)))+geom_line(color="green")+
xlab("Conditional correlation for SHY and LQD")+ylab("")
plot_grid(p1,p2,p3,labels="",nrow=3)
## Simulate log-returns
DCC_sim<-function(mfit,h,MC,seed){
#Input: mfit, result of the DCC estimation for k returns
#Output: MCxk matrix of cumulated returns for horizon h
m.sim=MC
n.sim=h
k=length(mfit@model$modeldata$asset.names)
sims=dccsim(mfit, n.sim = n.sim, m.sim=m.sim,
startMethod = c("unconditional", "sample")[2],
rseed=seed)
Ysim2=t(sapply(sims@msim$simX, FUN = function(x) x))#m.sim x (k*n.sim)
mat=kronecker(diag(k), matrix(1,ncol=1,nrow=n.sim))
Ysim3=Ysim2%*%mat
return(Ysim3)
}
NumR <- 720
YY <- exp(DCC_sim(fit,h=21,MC=10000,seed=NumR)) #with thus function  we return to our prices
getwd()
#source("optimization_problem.R")
#source("monthly log normal claims distr correct.R")
al <- 0.995
pars <- fit_claims$estimate #log mean e log standard deviation
premium <- 1.1*exp(pars[1]+0.5*(pars[2]^2)) #media della log normale
#premium=0
RP_problem(alpha=al,premium=premium,
roc=0,scenarios=YY,
parameters=pars,
type="NO_ROC")
#quindi qui mi devo trovare i pesi del portafoglio
#non trova piu portafogli che soddisfano l'equazione
num.pts = 7 #simul sette punti e costruisco la frontiera
res <- efficient_frontier_RP(num.pts=num.pts,step=0.00005,
scenarios=YY,
premium=premium,
parameters=pars,
alpha=al)
plot(res$Capital,res$ROC,type="l",
xlab="Optimal Capital",ylab="Expected Return on Capital",
xlim=range(res$Capital),ylim=range(res$ROC),col="red",lwd=2,
main="RP-constrained")
fit_claims$estimate[1]
tr <- MedMal$MedMalPaid # loss + lae (loss adjustment expenses) #cumulative triangle
library(ChainLadder)
library(fitdistrplus)
tr <- MedMal$MedMalPaid # loss + lae (loss adjustment expenses) #cumulative triangle
plot(tr)
perc_df <- 1/rev(cumprod(rev(c(attr(ata(tr), "vwtd"),1)))) #here we compute the annual development factors
perc_df <- unname(perc_df)    #what we can do with development factors is to understand
#to underatand throught time the change in the loss
plot(c(0, perc_df), type = "b")
inc <- c(perc_df[1]/12, sapply(2:length(perc_df), function(x) (perc_df[x] - perc_df[x - 1])/12))
#in this case we are looking at the change month for month of the development factors for incremental triangle
plot(inc, type = "b")
perc_inc <- c(rep(inc, each = 12))
perc_m <- cumsum(c(rep(inc, each = 12)))  #in questo modo calcoliamo i dev factors per ogni mese
plot(c(0, perc_m), type = "b")
dev_m <- 1/perc_m
plot(dev_m, type = "b")  #link ratios
df_m <- c(dev_m[1:length(dev_m) - 1] / dev_m[2:length(dev_m)], 1) #prende tutti i dev da 1 a 96 escludendo il primo
plot(df_m, type = "b") #shows the observations link ratios that are decreasing
# Bootstrap ---------------------------------------------------------------
n = 1000
bs_sim <- BootChainLadder(tr, R = n)
#sum(getLatestCumulative(matrix(bs_sim$simClaims[,,1]*rev(inc),ncol=8,nrow =8,byrow = TRUE)))
dist_mensile <- sapply(1:n, function(x){
sum(getLatestCumulative(bs_sim$simClaims[,,x]) * rev(inc))
})
hist(dist_mensile, nclass = 50)
fit_claims <- fitdist(dist_mensile, distr = "lnorm")
plot(fit_claims)
ks.test(dist_mensile, "plnorm", fit_claims$estimate[1], fit_claims$estimate[2])
fit_claims$estimate[1]
fit_claims$estimate[2]
indici_train <- sample(1:n, .80*n)
train <- dist_mensile[indici_train]
out_of_sample <- dist_mensile[-indici_train]
fit_claims_out=fitdist(out_of_sample,distr = "lnorm")
plot(fit_claims_out)
ks.test(out_of_sample, "plnorm", fit_claims_out$estimate[1], fit_claims_out$estimate[2])
tr <- MedMal$MedMalPaid/1000 # loss + lae (loss adjustment expenses) #cumulative triangle
tr <- MedMal$MedMalPaid/1000 # loss + lae (loss adjustment expenses) #cumulative triangle
plot(tr)
perc_df <- 1/rev(cumprod(rev(c(attr(ata(tr), "vwtd"),1)))) #here we compute the annual development factors
perc_df <- unname(perc_df)    #what we can do with development factors is to understand
#to underatand throught time the change in the loss
plot(c(0, perc_df), type = "b")
inc <- c(perc_df[1]/12, sapply(2:length(perc_df), function(x) (perc_df[x] - perc_df[x - 1])/12))
#in this case we are looking at the change month for month of the development factors for incremental triangle
plot(inc, type = "b")
perc_inc <- c(rep(inc, each = 12))
perc_m <- cumsum(c(rep(inc, each = 12)))  #in questo modo calcoliamo i dev factors per ogni mese
plot(c(0, perc_m), type = "b")
dev_m <- 1/perc_m
plot(dev_m, type = "b")  #link ratios
df_m <- c(dev_m[1:length(dev_m) - 1] / dev_m[2:length(dev_m)], 1) #prende tutti i dev da 1 a 96 escludendo il primo
plot(df_m, type = "b") #shows the observations link ratios that are decreasing
# Bootstrap ---------------------------------------------------------------
n = 1000
bs_sim <- BootChainLadder(tr, R = n)
#sum(getLatestCumulative(matrix(bs_sim$simClaims[,,1]*rev(inc),ncol=8,nrow =8,byrow = TRUE)))
dist_mensile <- sapply(1:n, function(x){
sum(getLatestCumulative(bs_sim$simClaims[,,x]) * rev(inc))
})
hist(dist_mensile, nclass = 50)
fit_claims <- fitdist(dist_mensile, distr = "lnorm")
plot(fit_claims)
ks.test(dist_mensile, "plnorm", fit_claims$estimate[1], fit_claims$estimate[2])
fit_claims$estimate[1]
fit_claims$estimate[2]
indici_train <- sample(1:n, .80*n)
train <- dist_mensile[indici_train]
out_of_sample <- dist_mensile[-indici_train]
fit_claims_out=fitdist(out_of_sample,distr = "lnorm")
plot(fit_claims_out)
ks.test(out_of_sample, "plnorm", fit_claims_out$estimate[1], fit_claims_out$estimate[2])
RP_problem <- function(alpha,premium,roc,
scenarios,parameters,
type,x0=NULL){
require(nloptr)
func1 <- function(R,mu,sigma){
# This function computes Phi((-log(R)+mu)/sigma)
valA <- (-log(R)+mu)/sigma
pnorm(valA)
} # end func1
func2 <- function(R,mu,sigma){
# This function computes dPhi((-log(R)+mu)/sigma)/dR
valA <- (-log(R)+mu)/sigma
-(1/R)*dnorm(valA)
} # end func2
J <- dim(scenarios)[2]
I <- dim(scenarios)[1]
pars <- parameters
al <- alpha
eval_f <- function(x) x[J+1]
eval_grad_f <- function(x) c(rep(0,J),1)
if(type=="NO_ROC"){
eval_g_ineq <- function(x){
valueA <- 0
for(i in 1:I){
valueB <- as.numeric(scenarios[i,]%*%x[1:J])
valueA <- valueA+func1(valueB,pars[1],pars[2])
}
valueA <- (valueA/I)-1+al
return(valueA)
}
eval_jac_g_ineq <- function(x){
gradient <- numeric(J+1)
for(i in 1:I){
valueB <- as.numeric(scenarios[i,]%*%x[1:J])
valueC <- func2(valueB,pars[1],pars[2])
gradient[1:J] <- gradient[1:J]+valueC*scenarios[i,]
}
gradient <- gradient/I
return(gradient)
}
} else if(type=="ROC"){
M <- colMeans(scenarios)
b <- exp(pars[1]+0.5*(pars[2]^2))
eval_g_ineq <- function(x){
valueA <- 0
for(i in 1:I){
valueB <- as.numeric(scenarios[i,]%*%x[1:J])
valueA <- valueA+func1(valueB,pars[1],pars[2])
}
valueA <- (valueA/I)-1+al
valueB <- b-x[1:J]%*%M+roc*x[J+1]
return(c(valueA,valueB))
}
eval_jac_g_ineq <- function(x){
gradient1 <- numeric(J+1)
for(i in 1:I){
valueB <- as.numeric(scenarios[i,]%*%x[1:J])
valueC <- func2(valueB,pars[1],pars[2])
gradient1[1:J] <- gradient1[1:J]+valueC*scenarios[i,]
}
gradient1 <- gradient1/I
gradient2 <- numeric(J+1)
gradient2[1:J] <- -M
gradient2[(J+1)] <- roc
return(rbind(gradient1,gradient2))
}
}
eval_g_eq <- function(x) sum(x[1:J])-x[J+1]-premium
eval_jac_g_eq <- function(x) c(rep(1,J),-1)
b <- exp(pars[1]+0.5*(pars[2]^2))
if(is.null(x0)) x0 <- c(b,rep(0,J-1),b)
lb <- rep(0,J+1)
ub <- rep(Inf,J+1)
opts <- list("algorithm"="NLOPT_LD_SLSQP",
"xtol_rel"=1.0e-8,
"maxeval"=2000,
"print_level"=2)
res <- nloptr(x0=x0,
eval_f=eval_f,
eval_grad_f = eval_grad_f,
lb=lb,ub=ub,
eval_g_ineq=eval_g_ineq,
eval_jac_g_ineq=eval_jac_g_ineq,
eval_g_eq=eval_g_eq,
eval_jac_g_eq = eval_jac_g_eq,
opts=opts)
capital <- res$solution[J+1]
assets <- res$solution[1:J]
return(list(Capital=capital,Assets=assets))
}
efficient_frontier_RP <- function(num.pts,step,scenarios,
premium,
parameters,
alpha){
al <- alpha
pars <- parameters
K <- num.pts
N <- dim(YY)[2]
sol <- RP_problem(alpha=al,premium=premium,
roc=0,scenarios=YY,
parameters=pars,
type="NO_ROC",x0=NULL)
M <- colMeans(YY)
roc <- ( (sol$Assets%*%M-exp(pars[1]+0.5*(pars[2]^2)))
/ sol$Capital )
points <- matrix(0,nrow=2,ncol=K)
assets <- matrix(0,nrow=K,ncol=N)
for(i in 1:K){
x0 <- c(sol$Assets,sol$Capital)
sol <- RP_problem(alpha=al,premium=premium,
roc=roc,scenarios=YY,
parameters=pars,
type="ROC",x0=x0)
points[1,i] <- roc
points[2,i] <- sol$Capital
assets[i,] <- sol$Assets
roc <- roc + step
}
return(list(Capital=points[2,],ROC=points[1,],Assets=assets))
}
library(quantmod)
asset.names <- c("^GSPC","SHY","LQD")
getSymbols(Symbols=asset.names,from="2005-01-03",to="2011-07-29",
src="yahoo")
Prices <- cbind(GSPC$GSPC.Adjusted,
SHY$SHY.Adjusted,
LQD$LQD.Adjusted)
names(Prices) <- c("GSPC","SHY","LQD")
head(Prices,2)
tail(Prices,2)
library(PerformanceAnalytics)
Rt <- Return.calculate(Prices,method="log")[-1,]
mu <- apply(Rt,2,mean)
sig <- apply(Rt,2,sd)
library(e1071)
sk <- apply(Rt,2,skewness)
ku <- 3+apply(Rt,2,kurtosis) #under the normality assumption the kurtosis of a distribution is
#compared with the tail of the normal distribution
#the function apply gives us the possibility to apply the min and the max function
mi <- apply(Rt,2,min)
ma <- apply(Rt,2,max)
library(rmgarch)
spec <- ugarchspec(mean.model=list(armaOrder=c(0,0),
include.mean=FALSE),
variance.model=list(model='sGARCH'))
mgarch.spec <- dccspec(multispec(replicate(3,spec )) )
#split the sample
Y <- Rt["2005-01-03::2009-12-31"]
fit<-dccfit(mgarch.spec,data=Y)
fit
## plot conditional variances and conditional correlations
library(ggplot2)
library(dplyr)
library(hrbrthemes)
# install.packages("dplyr")
da1 <- rcov(fit)['GSPC','GSPC',]
da1 <- xts(da1,order.by=as.Date(names(da1)))
p1 <- ggplot(da1,aes(x=index(da1),y=coredata(da1)))+geom_line(color="red")+
xlab("Conditional variance for S&P 500")+ylab("")
da2 <- rcov(fit)['SHY','SHY',]
da2 <- xts(da2,order.by=as.Date(names(da2)))
p2 <- ggplot(da2,aes(x=index(da2),y=coredata(da2)))+geom_line(color="blue")+
xlab("Conditional variance for SHY")+ylab("")
da3 <- rcov(fit)['LQD','LQD',]
da3 <- xts(da3,order.by=as.Date(names(da3)))
p3 <- ggplot(da3,aes(x=index(da3),y=coredata(da3)))+geom_line(color="green")+
xlab("Conditional variance for LQD")+ylab("")
library(cowplot)
plot_grid(p1,p2,p3,labels="",nrow=3)
rm(p1,p2,p3) #rimuove le tre componenti p1,p2,p3
da1 <- rcor(fit,type="R")['GSPC','SHY',]
da1 <- xts(da1,order.by=as.Date(names(da1)))
p1 <- ggplot(da1,aes(x=index(da1),y=coredata(da1)))+geom_line(color="red")+
xlab("Conditional correlation for S&P 500 and SHY")+ylab("")
da2 <- rcor(fit,type="R")['GSPC','LQD',]
da2 <- xts(da2,order.by=as.Date(names(da2)))
p2 <- ggplot(da2,aes(x=index(da2),y=coredata(da2)))+geom_line(color="blue")+
xlab("Conditional correlation for S&P 500 and LQD")+ylab("")
da3 <- rcor(fit,type="R")['SHY','LQD',]
da3 <- xts(da3,order.by=as.Date(names(da3)))
p3 <- ggplot(da3,aes(x=index(da3),y=coredata(da3)))+geom_line(color="green")+
xlab("Conditional correlation for SHY and LQD")+ylab("")
plot_grid(p1,p2,p3,labels="",nrow=3)
## Simulate log-returns
DCC_sim<-function(mfit,h,MC,seed){
#Input: mfit, result of the DCC estimation for k returns
#Output: MCxk matrix of cumulated returns for horizon h
m.sim=MC
n.sim=h
k=length(mfit@model$modeldata$asset.names)
sims=dccsim(mfit, n.sim = n.sim, m.sim=m.sim,
startMethod = c("unconditional", "sample")[2],
rseed=seed)
Ysim2=t(sapply(sims@msim$simX, FUN = function(x) x))#m.sim x (k*n.sim)
mat=kronecker(diag(k), matrix(1,ncol=1,nrow=n.sim))
Ysim3=Ysim2%*%mat
return(Ysim3)
}
NumR <- 720
YY <- exp(DCC_sim(fit,h=21,MC=10000,seed=NumR)) #with thus function  we return to our prices
getwd()
#source("optimization_problem.R")
#source("monthly log normal claims distr correct.R")
al <- 0.995
pars <- fit_claims$estimate #log mean e log standard deviation
premium <- 1.1*exp(pars[1]+0.5*(pars[2]^2)) #media della log normale
#premium=0
RP_problem(alpha=al,premium=premium,
roc=0,scenarios=YY,
parameters=pars,
type="NO_ROC")
#quindi qui mi devo trovare i pesi del portafoglio
#non trova piu portafogli che soddisfano l'equazione
num.pts = 7 #simul sette punti e costruisco la frontiera
res <- efficient_frontier_RP(num.pts=num.pts,step=0.00005,
scenarios=YY,
premium=premium,
parameters=pars,
alpha=al)
plot(res$Capital,res$ROC,type="l",
xlab="Optimal Capital",ylab="Expected Return on Capital",
xlim=range(res$Capital),ylim=range(res$ROC),col="red",lwd=2,
main="RP-constrained")
source("C:/Users/Roberto/Desktop/Materiale lezioni private/Life proj/Data+Models_2022 BEL.R", echo=TRUE)
library(ChainLadder)
data(ABC)
cum.triangle <- ABC
inc.triangle <- cum2incr(cum.triangle)
#Data frame
d=as.data.frame(inc.triangle)
d=na.omit(d) #elimina na
d$origin=as.factor(d$origin)  #trasforma i dev e gli origin in fact presi prima come numeric
d$dev=as.factor(d$dev)
summary(d)
#replichiamo Chain Ladder:
glm_ChainLadder= glm(value~origin+dev+0,family =quasipoisson(link = "log"),data=d)
summary(glm_ChainLadder)
a=coef(glm_ChainLadder)["origin1987"]
v=coef(glm_ChainLadder)[12:21]
a+v
exp(a+v)
sum(exp(a+v))
MackChainLadder(ABC)
#glm produce gli stessi valori del chainladder
#calendar year
d$calyear=as.factor(as.numeric(d$origin)+as.numeric(d$dev)-1)
glm_ChainLadder= glm(value~origin+calyear+0,family =quasipoisson(link = "log"),data=d)
summary(glm_ChainLadder)
mean(d$value)
sum(d$value)/length(d$value)
var(d$value)
#replichiamo Chain Ladder:
glm_ChainLadder= glm(value~origin+dev+0,family =quasipoisson(link = "log"),data=d)
summary(glm_ChainLadder)
a=coef(glm_ChainLadder)["origin1987"]
v=coef(glm_ChainLadder)[12:21]
a+v
summary(glm_ChainLadder)
mean(glm_ChainLadder$fitted.values)
var(glm_ChainLadder$fitted.values)
#Verifichiamo che il GLM abbia fittato correttamente i dati
#mean(glm_ChainLadder$fitted.values)
#var(glm_ChainLadder$fitted.values)
summary(glm_ChainLadder)
a=coef(glm_ChainLadder)["origin1987"]
v=coef(glm_ChainLadder)[12:21]
a
v
a=coef(glm_ChainLadder)
a
glm_ChainLadder$
a=coef(glm_ChainLadder)["origin1987"]
glm_ChainLadder$formula
glm_ChainLadder$residuals
glm_ChainLadder$effects
glm_ChainLadder$R
#Verifichiamo che il GLM abbia fittato correttamente i dati
#mean(glm_ChainLadder$fitted.values)
#var(glm_ChainLadder$fitted.values)
summary(glm_ChainLadder)
glm_ChainLadder$R
a=coef(glm_ChainLadder)["origin1987"]
v=coef(glm_ChainLadder)[12:21]
a+v
exp(a+v)
sum(exp(a+v))
MackChainLadder(ABC)
MackChainLadder(ABC)[,2]
fm=MackChainLadder(ABC)
fm$
#glm produce gli stessi valori del chainladder
#calendar year
d$calyear=as.factor(as.numeric(d$origin)+as.numeric(d$dev)-1)
fm$FullTriangle
plot(fm$f.se)
load("C:/Users/Roberto/Downloads/TermInsPortSima_5bc2c0bf3099c938e4c31a78468c216a.RData")
library(ChainLadder)
data(ABC)
cum.triangle <- ABC
inc.triangle <- cum2incr(cum.triangle)
#Data frame
d=as.data.frame(inc.triangle)
d=na.omit(d) #elimina na
d
d$dev=as.factor(d$dev)
summary(d)
#replichiamo Chain Ladder:
glm_ChainLadder= glm(value~origin+dev+0,family =quasipoisson(link = "log"),data=d)
value
origin
summary(glm_ChainLadder)
a=coef(glm_ChainLadder)["origin1987"]
v=coef(glm_ChainLadder)[12:21]
a=coef(glm_ChainLadder)["origin1987"]
a
v
a+v
exp(a+v)
sum(exp(a+v))
MackChainLadder(ABC)
#glm produce gli stessi valori del chainladder
#calendar year
d$calyear=as.factor(as.numeric(d$origin)+as.numeric(d$dev)-1)
glm_ChainLadder= glm(value~origin+calyear+0,family =quasipoisson(link = "log"),data=d)
summary(glm_ChainLadder)
d
getwd()
setwd("C:\Users\Roberto\Desktop\Master in AI e DS\Programmazione con R")
setwd("C:/Users/Roberto/Desktop/Master in AI e DS/Programmazione con R")
faglie <- st_read("gem_active_faults.shp")
library(readr)
library(sf)
library(ggplot2)
library(dplyr)
library(maps)
library(tidyr)
library(RColorBrewer)
library(stringr)
library(rnaturalearth)
library(rnaturalearthdata)
library(ggrepel)
library(lubridate)
library(gridExtra)
library(tidyr)
library(corrplot)
library(ggcorrplot)
faglie <- st_read("gem_active_faults.shp")
terremoti <- read.csv("Earthquake_1970-2025.csv")
terremoti <- read.csv("Earthquake_1970-2025.csv")
setwd("C:/Users/Roberto/Desktop/Progetto in R. L-D-R")
terremoti <- read.csv("Earthquake_1970-2025.csv")
faglie <- st_read("gem_active_faults.shp")
faglie <- st_read("gem_active_faults.shp")
faglie <- st_read("gem_active_faults.shp")
terremoti <- read.csv("Earthquake_1970-2025.csv")
terremoti <- read.csv("Earthquake_1970-2025.csv")
